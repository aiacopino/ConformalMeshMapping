
\chapter{Notation and Preliminaries}\label{ch:notation}

\section*{Definitions and Notation}

\subsection*{Arrays and Median}
Throughout this thesis, we denote the input array of size \(n\) by
\[
a = (a[1], a[2], \dots, a[n]).
\]
We may also use $a[1 \dots n]$ to denote an array $a$ of length $n$.
Our goal is to find the \emph{median} of \(a\). Formally, we define:
\begin{itemize}
    \item If \(n\) is odd, the median is the element in the \(\lceil n/2 \rceil\)-th position of the array when sorted in non-decreasing order.
    \item If \(n\) is even, the median is the element in the \(\lceil n/2 \rceil\)-th position of the sorted array.
\end{itemize}

\subsection*{Random Variables and Expectation}
In the analysis of our randomized median-finding algorithm, we shall frequently refer to \emph{random variables}. A random variable \(X\) is a function from the sample space \(\Omega\) (representing all possible outcomes of a probabilistic experiment) to the real numbers \R.

\paragraph{Expectation.}
The \emph{expected value}, or \emph{mean}, of \(X\) is defined by
\[
\E{X} = \sum_{\omega \in \Omega} X(\omega)\,\Prob{\omega},
\]
in the discrete case. If \(X\) has a probability density function \(f_X\) (in the continuous case), then
\[
\E{X} = \int_{-\infty}^{\infty} x \, f_X(x) \, dx.
\]
Intuitively, \(\E{X}\) indicates the “average” or “central” value of the random variable over many independent trials.

\subsection*{Variance and Standard Deviation}
To measure how much a random variable deviates from its mean, we use its \emph{variance}, denoted \(\Var{X}\). This is defined as:
\[
\Var{X} = \E{\bigl(X - \E{X}\bigr)^2}.
\]
The \emph{standard deviation} of \(X\), denoted \(\sigma_X\), is the square root of the variance:
\[
\sigma_X = \sqrt{\Var{X}}.
\]
A smaller variance (or standard deviation) means the random variable’s values tend to lie closer to its mean, whereas a larger variance indicates a broader spread of values.

\subsection*{Asymptotic Notation}
In describing the time complexity of algorithms, we use \Oh*{\dots} notation in a standard way:
\[
f(n) = \Oh*{g(n)} 
\quad \Longleftrightarrow \quad
\lim_{n \to \infty} \frac{f(n)}{g(n)} < \infty.
\]
We may also use \Th{\ldots}, \Om{\ldots}, and other asymptotic notations where necessary.

\section*{Chebyshev’s Inequality}

\emph{Chebyshev’s inequality} is a cornerstone result in probability theory, providing a quantitative bound on how a random variable deviates from its expected value. Formally, let \(X\) be any random variable with finite mean \(\E{X}\) and finite variance \(\Var{X}\). Then, for any \(k > 0\),
\[
\Prob{\abs{X - \E{X}} \ge k} 
\;\le\; 
\frac{\Var{X}}{k^2}.
\]

\subsection*{Intuition and Relation to Markov’s Inequality}
To build intuition, recall the simpler \emph{Markov’s inequality}, which states that for any nonnegative random variable \(Y\) and \(t>0\),
\[
\Prob{Y \ge t} \le \frac{\E{Y}}{t}.
\]
Chebyshev’s inequality is often viewed as an application of Markov’s inequality to the random variable \((X - \E{X})^2\). Indeed,
\[
\Prob{(X - \E{X})^2 \ge k^2} 
\le 
\frac{\E{\bigl(X - \E{X}\bigr)^2}}{k^2}
= 
\frac{\Var{X}}{k^2},
\]
and from
\(\{\,|X - \E{X}| \ge k\,\} \,\equiv\, \{\,(X - \E{X})^2 \ge k^2\,\},\)
we obtain Chebyshev’s inequality.

The key takeaway is that Chebyshev’s inequality captures a broad class of distributions. It requires no assumption of normality or other specific distributional shapes, only finite variance. In exchange for this generality, Chebyshev’s bound can be somewhat loose compared to more specialized inequalities (e.g., Hoeffding’s, Chernoff’s, or Bernstein’s). Nonetheless, it remains a powerful and classic tool for bounding “tail events,” i.e.\ events in which \(X\) significantly deviates from its mean.

\subsection*{Why We Use Chebyshev’s Inequality Here}
In our randomized median-finding algorithm, we draw random samples from the input array and use them to approximate the true median. The “rank” of our chosen pivots in the sorted order of the entire array are themselves random variables. We want to show that, with high probability, these rank containt the \(\lceil n/2\rceil\)-th position.

By applying Chebyshev’s inequality, we obtain a bound on \Prob{\abs{X - \E{X}} \ge k} for a well-chosen \(k\), where \(X\) is the rank of our pivot.
In other words, Chebyshev’s inequality formally establishes the reliability of our sampling strategy by demonstrating that, except with small probability, our pivots partition the array around the actual median. This allows us to restrict further computation to a subset of the array whose size is guaranteed to be sufficiently small, thereby ensuring linear running time.

\subsection*{Extensions and Alternatives}
While Chebyshev’s inequality is sufficient for our purposes, more refined analyses might employ other concentration bounds such as Hoeffding’s or Chernoff’s inequalities. These stronger bounds can yield tighter probabilities when the underlying sample distribution has additional regularity properties (e.g., bounded random variables or independent Bernoulli trials). However, Chebyshev’s inequality has the advantage of simplicity and broad applicability, making it an ideal choice for illustrating the core ideas in a clear manner.
