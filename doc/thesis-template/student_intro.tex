
\chapter{Introduction}\label{ch:paper_introduction}

The median of a dataset is a fundamental measure of central tendency that arises in numerous computational and statistical contexts. In an array of \(n\) elements, indexed from 1 to \(n\), the median can be intuitively understood as the “middle” element in the sorted order if \(n\) is odd, or the average of the two middle elements if \(n\) is even. While this definition naturally suggests sorting the entire array, doing so in \Oh*{n \Log*{n}} time is often unnecessary when the goal is to identify only the median.

\section*{Motivation and Previous Work}

Several well-known algorithms exist for selecting the median or, more generally, the \(k\)-th smallest element of an array. A straightforward method is to sort the array and then pick the median in constant time. However, this approach has a runtime of \Oh*{n \Log*{n}}, which can be excessive for large \(n\). More advanced methods, such as the \emph{median-of-medians} algorithm introduced by Blum, Floyd, Pratt, Rivest, and Tarjan~\cite{blum1973time}, achieve a worst-case runtime of \Oh*{n} using a deterministic divide-and-conquer strategy. While optimal in the worst case, the median-of-medians algorithm involves significant overhead, making it less practical in many scenarios. Alternatively, randomized algorithms like Quickselect often achieve linear-time performance on average but can degrade to \Oh*{n^2} in the worst case without additional precautions.

\section*{Our Contribution}

In this thesis, we present a \emph{randomized} algorithm that computes the median in \Oh*{n} time \emph{with high probability}, thereby combining practical efficiency with strong theoretical guarantees. The key idea is to leverage a carefully chosen random sample of the input array:
\begin{enumerate}
    \item We extract a subset of elements (the “sample”) whose size grows sublinearly with \(n\).
    \item Using this sample, we identify pivot elements that, with high probability, closely approximate the true median's position.
    \item We then reduce the problem to sorting only a small subset of the original array, thereby achieving near-linear time complexity.
\end{enumerate}

Central to our analysis is the use of \textbf{Chebyshev’s inequality}. While often introduced in purely statistical contexts, Chebyshev’s inequality provides a useful probabilistic bound for ensuring that the sampled elements approximate the median accurately. In essence, it tells us that \Prob{\abs{X - \E{X}} \ge k} can be kept small if \Var{X} is finite, thereby giving us high confidence that our pivot-based “zoom-in” on the median is correct.

More concretely, we prove the following theorem with our algorithm.

\begin{restatable}{theorem}{mainTheoremCommand}
    \label{theorem:mainTheorem}
    There exists a randomized algorithm to compute the median that runs in time $\Oh*{n}$ and succeeds with probability $1 - \Oh*{n^{-1/4}}$.
\end{restatable}

\section*{Thesis Outline}

This thesis is structured as follows:
\begin{itemize}
    \item \textbf{\Cref{ch:notation} (Notation and Preliminaries):} We define our notational conventions and offer a brief refresher on Chebyshev’s inequality, setting the stage for the randomized analysis to come.
    \item \textbf{\Cref{ch:analysis} (Algorithm and Analysis):} We detail the randomized median-finding algorithm, present its pseudocode, and provide a rigorous proof of correctness. We also analyze the runtime and show why it remains \Oh*{n} with high probability.
    \item \textbf{\Cref{ch:conclusion} (Conclusion):} We summarize our findings, discuss the algorithm’s theoretical and practical implications, and highlight possible directions for future research.
\end{itemize}
